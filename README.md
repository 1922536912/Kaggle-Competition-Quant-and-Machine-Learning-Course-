# Welcome to Kaggle Competition - Quant & Machine Learning Course


## Part I: Machine Learning Essentials

### Week 1 (July 31- Aug 1)
- 1.1 Understand the problem: Competition & machine learning project overview
	- [How to win Kaggle competitions](https://docs.google.com/document/d/14KDMW_o1yflcZd4E0PSlKxzI68zdHG20Qz6X5wmkgSA/edit?usp=sharing)
	- [Slides](https://docs.google.com/presentation/d/1cYZACKaB7e2vRZAv8Oe1GcVy6U_xBeOoeptJsl3KZtI/edit?usp=sharing)
	- [Kaggle Notebook #1: Introduction to competition problem & data](https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data)
	- [Python Tutorial #1: Basics Numpy](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_1_basic_numpy.ipynb)
	- [Python Tutorial #2: Basics Pandas](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_2_basic_pandas.ipynb)
- 1.2 Explore the data: basic python tutorial - data summary & missing data
	- [Slides](https://docs.google.com/presentation/d/1Dwzv1t2ZEr7j9I6jOQPxMv0hqFkb91VhaRGmIkUAb-Q/edit?usp=sharing)
	- [Python Tutorial #3: Basic data processing and visualization](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_3_data_preprocessing_visualization.ipynb)
### Week 2 (Aug 7-8)
- 2.1 EDA
	- [Slides](https://docs.google.com/presentation/d/13gwvLolY0Ug_WKROeVYpHpblWhNhvmj3DskSxsu3Ta0/edit?usp=sharing)
	- [Kaggle Notebook #2: EDA tutorial for competition data](https://www.kaggle.com/gunesevitan/optiver-realized-volatility-prediction-eda)
	- [Key concepts in machine learning](https://towardsdatascience.com/machine-learning-basics-part-1-a36d38c7916)
- 2.2 Feature Engineering
	- [Slides](https://docs.google.com/presentation/d/1R8DDZf6qIG2eKTtfGcW6kph-fpTm6m3NyQkYVNk77rg/edit?usp=sharing)
	- [Python Tutorial #4: Feature engineering](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_4_feature_engineering.ipynb)


### Week 3 (Aug 14-15)
- 3.1 Cross validation, grid search for parameter selection
	- [Python Tutorial #5: Cross validation](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_5_cross_validation.ipynb) 
- 3.2 Key machine learning models: Linear regression, Ridge, Lasso models
	- [Python Tutorial #6: Linear models](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_6_linear%20models.ipynb)

### Week 4 (Aug 21-22)
- 4.1 Key machine learning models: Tree based models
	- [Python Tutorial #7: Decision Trees](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_7_decision_tree.ipynb)
	- [Gini impurity in decision tree CART algorithm](https://victorzhou.com/blog/gini-impurity/)
- 4.2 Gradient boost models: Xgboost, LightGBM
	- [Python Tutorial #8: Xgboost & LightGBM](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_8_xgboost_LightGBM.ipynb)
	- [Gradient boosting explained](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

### Week 5 (Aug 28-29)
- 5.1 Competition data LightGBM notebook
	- [Kaggle Notebook #3: Feature engineering and LightGBM](https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea)
- 5.2 Improving performance by stacking multiple ML models together
	- [Kaggle Notebook #4: LightGBM with optimized params](https://www.kaggle.com/felipefonte99/optiver-lgb-with-optimized-params)
	- [Python Tutorial #9: Stacking multiple ML models](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_9_stacking_models.ipynb)

**Milestone #1: Submission a machine learning model**


## Part II: Advanced Modeling and Techniques

### Week 6 (Sep 4-5)
- 6.1 Deep neural network
	- [Slides](https://docs.google.com/presentation/d/1XcIfz7TBiMcuJSdHLzdzekB-GxCbHLu9V01tM3H9PFU/edit#slide=id.p)
	- [Python Tutorial #10: Deep NN](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_10_deep_NN.ipynb)
- 6.2 Competition NN stock embedding notebook
	- [Kaggle Notebook #5: NN start with stock embedding](https://www.kaggle.com/lucasmorin/tf-keras-nn-with-stock-embedding)
	- [Strategy to win this Kaggle competition](https://docs.google.com/presentation/d/10Bv9fKUyFlGtV0pw07VUZPJElguDjxDIvVObzXDzaRA/edit#slide=id.gec9923c869_0_0)
### Week 7 (Sep 11-12)
- 7.1 Combine ML models via ML model
	- [Slides](https://docs.google.com/presentation/d/1DBp7sNM__CKd38C6QdATjQhdj9lK5DD6UkZqx2FZ7gI/edit#slide=id.gebaeaeb46d_0_39)
	- [Python Tutorial #11: Combine models by ML]
	- [Kaggle Notebook #6: NN + LightGBM](https://www.kaggle.com/mayangrui/lgbm-ffnn)
- 7.2 Combine ML models via weighted average
	- [Python Tutorial #12: Combine models by weighted average]
### Week 8 (Sep 18-19)
- 8.1 Deep neural network - LSTM
- 8.2 LightGBM parameter tuning
### Week 9 (Sep 25-26) - team project Q&A

**Milestone #2: Final Submission on Sep 26**
